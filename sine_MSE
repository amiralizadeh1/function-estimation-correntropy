#predict sine function + monte carlo

np_seed = 5
monte_carlo_iterations = 10 ## number of monte carlo iterations. the larger, the more accurate results
n_points = 600
scale = 0.03  ## this parameter determines the magnitude of heavy-tailed noise distributions

#import tensorflow as tf
import numpy as np
np.random.seed(np_seed)
#import pyrennn_mcc as prn
import pyrennn as prn
import winsound
import time
import pylab as pl
import shuffler as sh
import snr_calc as s
start = time.time()

snrr = []
for k1 in range(monte_carlo_iterations):
    snrr.append(0)
    
history = []
for k1 in range(monte_carlo_iterations):
    history.append(0)

yn= []
for k1 in range(monte_carlo_iterations):
    yn.append(0)
    
y_out = []
for k1 in range(monte_carlo_iterations):
    y_out.append(0)

test_y = []
for k1 in range(monte_carlo_iterations):
    test_y.append(0)

def alt(a): # this function selects some elements of a list
    x = np.arange(0,len(a),2)
    return np.take(a,x)
def alt1(a): # this function selects the other elements of a list that were not selected by alt(.)
    y= np.delete(np.arange(0,len(a)),np.arange(0,len(a),2))
    return np.take(a,y)

def nn(N_POINTS,SCALE): ## this function is the main body of the project which is run 'monte_carlo_iterations' times.
    x = np.linspace(-2.0 * np.pi, 2.0 * np.pi,  n_points).reshape(1, -1)
    y = np.sin(x)
    n = np.random.standard_cauchy(n_points).reshape(1, -1) * scale
#    n = np.random.exponential(0.2,n_points).reshape(1, -1)
#    n = np.random.normal(0,0.01,n_points).reshape(1,-1)
    yn = y + n

    ## data samples are put together in a 3*n_points matrix
    data=np.append(x,y,axis=0)
    data=np.append(data,yn,axis=0)
    
    ## split data to train and test datasets of equal size.
    train_x=alt(data[0,:])
    train_y=alt(data[1,:])
    train_yn=alt(data[2,:])
    test_x=alt1(data[0,:])
    test_y=alt1(data[1,:])
    
    ## shuffle data samples
    sh.shuffle(train_x)
    sh.shuffle(train_y)
    sh.shuffle(train_yn)

    ## create and parameterize the levenberg-marquardt NN
    net = prn.CreateNN([1,10,10,1]) ## set number of layers and number of neurons and specify the bandwidth of correntropy
    net= prn.train_LM(train_x,train_yn,net,verbose=False,k_max=100,E_stop=1e-9,dampfac=3.0,dampconst=10,min_E_step=1e-9)
    y_out = prn.NNOut(test_x,net)
    
    
    print(f'length of training data is {len(train_x)} and length of testing data is {len(test_x)}')
    
    ##plot noisy data
    pl.plot(yn[0])
    pl.xlabel('Samples')
    pl.ylabel('Values')
    pl.title('Noisy Data for Training and testing')
    pl.legend(['Noisy Data'],loc='upper right')
    #pl.savefig('plot.png',dpi=300,bbox_inches='tight')
    pl.show()

    ##compare NN output and expected output for this iteration
    pl.plot(y_out)
    pl.plot(test_y)
    pl.xlabel('Samples')
    pl.ylabel('Values')
    pl.title('Comparing Neural Network Output with Target Data')
    pl.legend(['Neural Network Output','Target Data'],loc='upper right')
    pl.ylim([-2,2])
    #pl.savefig('plot.png',dpi=300,bbox_inches='tight')
    pl.show()

    ## plot the Difference Between NN Output and Expected output
    pl.plot(y_out-test_y)
    pl.xlabel('Samples')
    pl.ylabel('Values')
    pl.title('Difference Between NN Output and Expected output')
    pl.legend(['Difference'],loc='upper right')
    #pl.savefig('plot.png',dpi=300,bbox_inches='tight')
    pl.show()
    
    print('snr')
    SNR = s.snr(y[0],n[0])
    print(s.snr(y[0],n[0]))
    
    return net['ErrorHistory'] ,yn[0].tolist(),y_out.tolist(),test_y.tolist(),SNR



## for each iteration, DIFFERENT data and noise samples are fed to a DIFFERENT NN weights        
for k in range(monte_carlo_iterations):
    print(f'MONTE CARLO NUMBER {k+1}:')
    history[k],yn[k],y_out[k],test_y[k],snrr[k] = nn(n_points,scale)
    print(f'MONTE CARLO NUMBER {k+1} DONE')
 
## calculate mean results
snrr_mean = (np.mean(snrr)) ## mean of snr over all iterations
yn_mean = (np.mean(yn,0)) ## mean of noisy target data over all iterations
y_out_mean = (np.mean(y_out,0)) ## mean of NN output data over all iterations
test_y_mean = (np.mean(test_y,0)) ## mean of test data over all iterations

print(f'mean resuts over {monte_carlo_iterations}:')  

## mean of noisy targets over all iterations
pl.plot(yn_mean)
pl.xlabel('Samples')
pl.ylabel('Values')
pl.title('mean noise')
pl.legend(['mean noise'],loc='upper right')
#pl.savefig('plot.png',dpi=300,bbox_inches='tight')
pl.show()

## mean results over all iterations for comparing NN output and expected output samples
pl.plot(y_out_mean)
pl.plot(test_y_mean)
pl.xlabel('Samples')
pl.ylabel('Values')
pl.title('Comparing Neural Network Output with Target Data')
pl.legend(['Neural Network Output by MSE_LM','Target Data'],loc='lower right')
pl.ylim([-2,2])
pl.savefig('sine_mse.jpg',dpi=300,bbox_inches='tight')
pl.show()

## plot learning history
historyy=[]
for k in range(monte_carlo_iterations):
    if len(history[k])==100: ## this condition helps exclude early-stopped iterations from overal history
        historyy.append(history[k])
history_mean = (np.mean(historyy,0))  ## mean results over all iterations

pl.plot(history_mean)
pl.xlabel('epochs')
pl.ylabel('mse')
pl.title('learning curve')
pl.legend(['error history'],loc='upper right')
pl.savefig('history_mse.jpg',dpi=300,bbox_inches='tight')
pl.ylim([0,100])
pl.show()

print(f'snrr_mean is {snrr_mean}')
E=sum((test_y_mean-y_out_mean)**2)
print(f'E is {E}')

end = time.time()
print(f"Runtime of the program is {end - start}")
winsound.Beep(1000,700)
